{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini project 2 - Data Cleansing Practice on Zillow Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week, we’ll practice how to do regular cleansing in Python with a real-world dataset – Zillow dataset, which is available in the [Zillow Prize: Zillow’s Home Value Prediction (Zestimate)](https://www.kaggle.com/c/zillow-prize-1). This Zillow dataset contains abundant missing data and will provide you a good environment to practice your skills on data cleaning. The cleansing of this dataset will be a great start for you to further play with this Kaggle Competition if you have more interest and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step-by-step mini project will illustrate you various ways to impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start with importing essentials\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read the train set and property set of Zillow dataset, and name them as train and properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Merge train and properties to one dataframe on parcelid and call it as df_train. Drop the column of 'parcelid' and 'transactiondate'. Check the first 5 rows to see how this merged dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.  (a) Generate a dataframe called missing_df from df_train, in which there are two columns, one is the column names of our features, the other column is the missing_count (the number of missing values) of that feature. The table should be ordered by missing_count decendingly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.(b) Draw a horizontal bar plot to visualize it. Following is an example to show how this figure may look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"missing_value_barplot.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Generate the correlation matrix for all the numerical features, and plot it by using heatmap or related visualization methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. From the results from Step 4, please list those features having a strong correlation. Generate a list called dropcols, and put those redundent variables into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Some variables where it is NA can be considered as the object does not exist. Such as 'hashottuborspa', if it is NA, we can assume the house doesn't contain the hot tub or spa. So we need to fix this kind of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Fix the hashottuborspa variable, fill the na part as None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Assume if the pooltype id and its related features is null then pool/hottub doesn't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) taxdeliquencyflag - assume if it is null then doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) If Null in garage count (garagecarcnt) it means there are no garages, and no garage means the size (garagetotalsqft) is 0 by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. There are more missing values in the 'poolsizesum' than in 'poolcnt'. Fill in median values for poolsizesum where pool count is >0 and missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. The number of missing value of 'fireplaceflag' is more than the 'fireplacecnt'. So we need to mark the missing 'fireplaceflag' as Yes when fireplacecnt>0, then the rest of 'fireplaceflag' should be marked as No. Then for the missing part in fireplacecnt, we can consider the number of fire place is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Fill some features with the most common value for those variables where this might be a sensible approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) AC Type (airconditioningtypeid)- Mostly 1's, which corresponds to central AC. It is reasonable to assume most other properties where this feature is missing are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) heating or system (heatingorsystemtypeid)- Mostly 2, which corresponds to central heating so seems reasonable to assume most other properties have central heating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. If the features where missing proportion is too much, we can directly delete them. Here we set 97% as our threshold (This is subjective) and add them into the dropcols. Then drop those features in dropcols from the full table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. We can also use some machine learning algorithm to fill the missing data. (you can skip this if you feel so difficult)\n",
    "In this dataset, there's quite a few variables which are probably dependant on longtitude and latitude data. It is reasonable to fill in some of the missing variables using geographically nearby properties (by using the longtitude and latitude information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't need to understand the following code, just consider this as a function you can use directly. The following code comes from the link:\n",
    "https://www.kaggle.com/auroralht/restoring-the-missing-geo-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "## Works on categorical feature\n",
    "def fillna_knn( df, base, target, fraction = 1, threshold = 10, n_neighbors = 5 ):\n",
    "    assert isinstance( base , list ) or isinstance( base , np.ndarray ) and isinstance( target, str ) \n",
    "    whole = [ target ] + base\n",
    "    \n",
    "    miss = df[target].isnull()\n",
    "    notmiss = ~miss \n",
    "    nummiss = miss.sum()\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    X_target = df.loc[ notmiss, whole ].sample( frac = fraction )\n",
    "    \n",
    "    enc.fit( X_target[ target ].unique().reshape( (-1,1) ) )\n",
    "    \n",
    "    Y = enc.transform( X_target[ target ].values.reshape((-1,1)) ).toarray()\n",
    "    X = X_target[ base  ]\n",
    "    \n",
    "    print( 'fitting' )\n",
    "    n_neighbors = n_neighbors\n",
    "    clf = neighbors.KNeighborsClassifier( n_neighbors, weights = 'uniform' )\n",
    "    clf.fit( X, Y )\n",
    "    \n",
    "    print( 'the shape of active features: ' ,enc.active_features_.shape )\n",
    "    \n",
    "    print( 'predicting' )\n",
    "    Z = clf.predict(df.loc[miss, base])\n",
    "    \n",
    "    numunperdicted = Z[:,0].sum()\n",
    "    if numunperdicted / nummiss *100 < threshold :\n",
    "        print( 'writing result to df' )    \n",
    "        df.loc[ miss, target ]  = np.dot( Z , enc.active_features_ )\n",
    "        print( 'num of unperdictable data: ', numunperdicted )\n",
    "        return enc\n",
    "    else:\n",
    "        print( 'out of threshold: {}% > {}%'.format( numunperdicted / nummiss *100 , threshold ) )\n",
    "\n",
    "#function to deal with variables that are actually string/categories\n",
    "def zoningcode2int( df, target ):\n",
    "    storenull = df[ target ].isnull()\n",
    "    enc = LabelEncoder( )\n",
    "    df[ target ] = df[ target ].astype( str )\n",
    "\n",
    "    print('fit and transform')\n",
    "    df[ target ]= enc.fit_transform( df[ target ].values )\n",
    "    print( 'num of categories: ', enc.classes_.shape  )\n",
    "    df.loc[ storenull, target ] = np.nan\n",
    "    print('recover the nan value')\n",
    "    return enc\n",
    "\n",
    "### Example: \n",
    "### If you want to impute buildingqualitytypeid with geological information:\n",
    "\"\"\"\n",
    "fillna_knn( df = df_train,\n",
    "                  base = [ 'latitude', 'longitude' ] ,\n",
    "                  target = 'buildingqualitytypeid', fraction = 0.15, n_neighbors = 1 )\n",
    "\"\"\"\n",
    "\n",
    "## Works on regression\n",
    "def fillna_knn_reg( df, base, target, n_neighbors = 5 ):\n",
    "    cols = base + [target]\n",
    "    X_train = df[cols]\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True).fit(X_train[base].values.reshape(-1, 1))\n",
    "    rescaledX = scaler.transform(X_train[base].values.reshape(-1, 1))\n",
    "\n",
    "    X_train = rescaledX[df[target].notnull()]\n",
    "    Y_train = df.loc[df[target].notnull(),target].values.reshape(-1, 1)\n",
    "\n",
    "    knn = KNeighborsRegressor(n_neighbors, n_jobs = -1)    \n",
    "    # fitting the model\n",
    "    knn.fit(X_train, Y_train)\n",
    "    # predict the response\n",
    "    X_test = rescaledX[df[target].isnull()]\n",
    "    pred = knn.predict(X_test)\n",
    "    df.loc[df_train[target].isnull(),target] = pred\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find out some features you can use this knn to fill the missing data, and use the above funtion to impute them**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
